# ğŸ§  Understanding `explainers.py` â€“ DeepLIFT-based Attribution for XAIguiFormer

This document explains how the module `explainers.py` works, focusing on the **DeepLIFT explainer** used to generate token-level importance scores for the XAI-guided attention mechanism in XAIguiFormer.

---

## ğŸ“Œ Scientific Background â€“ DeepLIFT (Shrikumar et al., 2017)

DeepLIFT (Deep Learning Important FeaTures) is an **XAI technique** that attributes output predictions to input features by **comparing activations to a reference (baseline)** and computing contributions based on differences.

Given:

- a model $f(x)$,
- an input $x$,
- and a baseline $x_0$ (e.g., all-zero input),

the contribution of each feature is:

$ \text{Contribution}_i = \frac{f(x) - f(x_0)}{x_i - x_{0,i}} \cdot (x_i - x_{0,i}) $


This gives per-feature relevance scores.

In XAIguiFormer, DeepLIFT is applied to the **vanilla transformer's classifier output** to identify which parts of the token sequence (connectome embeddings) are most important for prediction.

---

## ğŸ§© Code Breakdown â€“ `Explainer` Class

```python
from captum.attr import DeepLift

class Explainer:
    def __init__(self, model, classifier_head):
        self.model = model.eval()
        self.classifier = classifier_head.eval()

        def forward_fn(x):
            x_encoded = self.model(x)              # [B, F, d]
            x_pooled = x_encoded.mean(dim=1)       # mean over tokens
            logits = self.classifier(x_pooled)     # [B, C]
            return logits

        self.explainer = DeepLift(forward_fn)
```

- Wraps the **Vanilla Transformer + Coarse Classifier** as a function `forward_fn`.
- Applies mean pooling before classification.
- Captumâ€™s `DeepLift` is applied on this forward function.

---

```python
    def compute_explanations(self, x_input, target_labels):
        baseline = torch.zeros_like(x_input)  # all-zero input
        attributions = self.explainer.attribute(inputs=x_input,
                                                baselines=baseline,
                                                target=target_labels)
        return attributions
```

- Computes the attributions (importance scores) of each token in `x_input`.
- Outputs a tensor of the same shape: `attributions âˆˆ â„^{B Ã— F Ã— d}`.

---

## ğŸ–¼ï¸ Module Architecture

```
Explainer/
â”‚
â”œâ”€â”€ Input:
â”‚   â”œâ”€â”€ x_input: Tensor [B, Freq, d]       # e.g., q_rot or k_rot (before XAI attention)
â”‚   â””â”€â”€ target_labels: Tensor [B]          # ground-truth labels
â”‚
â”œâ”€â”€ Process:
â”‚   â”œâ”€â”€ Wrap forward pass of:
â”‚   â”‚     â†’ VanillaTransformerEncoder
â”‚   â”‚     â†’ MeanPooling over tokens
â”‚   â”‚     â†’ Classifier head
â”‚   â”œâ”€â”€ Instantiate Captum DeepLift
â”‚   â””â”€â”€ Compute attributions w.r.t. baseline (zero input)
â”‚
â””â”€â”€ Output:
    â””â”€â”€ attributions: Tensor [B, Freq, d]  # token-level feature importance
```

---

## âœ… Summary

- DeepLIFT identifies which **EEG frequency band tokens** (and which embedding dimensions) contribute the most to the vanilla transformer prediction.
- This importance map is injected into the **XAI-guided attention** as refined $Q$ and $K$.
- Enables explainable and focused attention, improving interpretability and performance.

---

This module operationalizes the key idea behind **Section 4.4 â€“ XAI Guided Self-Attention** in the XAIguiFormer paper, and connects the vanilla encoder to its explainable counterpart.
