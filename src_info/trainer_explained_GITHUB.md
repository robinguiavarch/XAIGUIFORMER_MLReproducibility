# ğŸ§  Understanding `trainer.py` â€“ End-to-End Forward Pipeline

This document explains the full architecture implemented in `trainer.py`, including how the different modules (dRoFE, Transformer, XAI, and loss) interact. It also includes a full input/output trace and model-level reasoning.

---

## ğŸ“Œ Overall Pipeline Structure

The `Trainer` class coordinates the entire XAIGuiFormer architecture:

1. Prepares Q/K using `dRoFE` to inject EEG frequency and demographic information.
2. Passes Q to a **Vanilla Transformer** to generate coarse attention representations.
3. Applies an XAI **explainer** (e.g. DeepLIFT) to extract token importance maps.
4. Uses these maps as Qexpl/Kexpl in a **XAI-Guided Transformer** to refine attention.
5. Produces two outputs: `logits_coarse`, `logits_refined`.
6. If labels are provided, computes a **joint supervised loss**.

---

## ğŸ§© Step-by-Step Architecture

```
Trainer/
â”‚
â”œâ”€â”€ Inputs:
â”‚   â”œâ”€â”€ x_raw        : Tensor [B, Freq, d]      # Token embeddings from Connectome Tokenizer
â”‚   â”œâ”€â”€ freq_bounds  : Tensor [Freq, 2]         # EEG band bounds [f_l, f_u]
â”‚   â”œâ”€â”€ age          : Tensor [B, 1]            # Patient age
â”‚   â”œâ”€â”€ gender       : Tensor [B, 1]            # Patient gender
â”‚   â””â”€â”€ y_true       : Tensor [B] or None       # Ground truth labels
â”‚
â”œâ”€â”€ Step 1 â€“ dRoFE Encoding:
â”‚   â”œâ”€â”€ q = W_q(x_raw), k = W_k(x_raw)
â”‚   â”œâ”€â”€ q_rot = dRoFE(q, freq_bounds, age, gender)
â”‚   â””â”€â”€ k_rot = dRoFE(k, freq_bounds, age, gender)
â”‚
â”œâ”€â”€ Step 2 â€“ Vanilla Transformer:
â”‚   â”œâ”€â”€ x_coarse = VanillaTransformer(q_rot)
â”‚   â””â”€â”€ logits_coarse = Classifier(mean(x_coarse))     # [B, C]
â”‚
â”œâ”€â”€ Step 3 â€“ XAI Explainer (TODO):
â”‚   â”œâ”€â”€ q_expl = DeepLIFT(x_coarse)
â”‚   â””â”€â”€ k_expl = DeepLIFT(x_coarse)
â”‚   âš ï¸ Currently mocked as q_expl = q_rot.detach(), k_expl = k_rot.detach()
â”‚
â”œâ”€â”€ Step 4 â€“ XAI-Guided Transformer:
â”‚   â”œâ”€â”€ x_refined = XAIGuidedTransformer(x_raw, q_expl, k_expl)
â”‚   â””â”€â”€ logits_refined = Classifier(mean(x_refined))   # [B, C]
â”‚
â”œâ”€â”€ Step 5 â€“ Joint Loss:
â”‚   â””â”€â”€ total_loss = (1 - alpha) * CE(logits_coarse, y_true) + alpha * CE(logits_refined, y_true)
â”‚
â””â”€â”€ Outputs:
    â”œâ”€â”€ If y_true is None:
    â”‚     â””â”€â”€ logits_coarse: [B, C]
    â”‚     â””â”€â”€ logits_refined: [B, C]
    â””â”€â”€ Else:
          â””â”€â”€ total_loss: scalar
```

---

## âœ… Key Design Insights

- Demographic encoding (dRoFE) allows the model to personalize frequency token embeddings.
- XAI guidance enforces **meaningful attention** based on saliency maps.
- The joint loss encourages **alignment** between coarse and refined predictions.

---

## âš ï¸ Remaining TODOs

- Integrate actual `ConnectomeTokenizer` output into `x_raw`
- Use a real `explainer.py` implementation (e.g., Captumâ€™s DeepLIFT)

---

This class is central to orchestrating the reproducibility of XAIGuiFormer from preprocessing to prediction and XAI-guided refinement.